{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DEEP - Assignment 1",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tylerelias/T-796-DEEP-Assignment/blob/main/DEEP_Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EETMIWmyKa4a"
      },
      "source": [
        "# Assignment\n",
        "\n",
        "- Jesper Winsten\n",
        "- Stefán Rangarsson\n",
        "- Tyler Elías Jones"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z6Y-X8ZePhXG"
      },
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "from os.path import join\n",
        "\n",
        "ROOT='/content/drive'\n",
        "drive.mount(ROOT)\n",
        "PROJECT='MyDrive/Colab Notebooks/Lab03'\n",
        "\n",
        "PROJECT_PATH=join(ROOT, PROJECT)\n",
        "\n",
        "!rsync -aP \"{PROJECT_PATH}\"/* ./"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg8IGCyfSGhZ"
      },
      "source": [
        "# Imports\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZt7eR7v09J4"
      },
      "source": [
        "def read_stock_prices(stocks):\n",
        "    with open('stock_prices.txt') as f:\n",
        "        for line in f:\n",
        "            fields = line.split()\n",
        "\n",
        "            year = 0\n",
        "            # Making the years into 0-2 format, put in dict later?\n",
        "            if fields[1] == \"2017\":\n",
        "                year = 0\n",
        "            if fields[1] == \"2018\":\n",
        "                year = 1\n",
        "            if fields[1] == \"2019\":\n",
        "                year = 2\n",
        "\n",
        "            record = (int(fields[0]),\n",
        "                      int(year),\n",
        "                      int(fields[2]),\n",
        "                      int(fields[3]),\n",
        "                      float(fields[4]))\n",
        "            stocks.append(record)\n",
        "\n",
        "\n",
        "def read_info(info):\n",
        "    with open('info.txt') as f:\n",
        "        for line in f:\n",
        "            fields = line.split()\n",
        "            record = (int(fields[0]),\n",
        "                      int(fields[1]),\n",
        "                      int(fields[2]),\n",
        "                      int(fields[3]),\n",
        "                      int(fields[4]),\n",
        "                      int(fields[5]),\n",
        "                      int(fields[6]),\n",
        "                      float(fields[7]),\n",
        "                      float(fields[8]),\n",
        "                      float(fields[9]),\n",
        "                      int(fields[10]),)\n",
        "            info.append(record)\n",
        "\n",
        "\n",
        "def read_segments(segments):\n",
        "    with open('market_segments.txt') as f:\n",
        "        for line in f:\n",
        "            fields = line.split()\n",
        "            record = (int(fields[0]),\n",
        "                      str(fields[1]),)\n",
        "            segments.append(record)\n",
        "\n",
        "\n",
        "def read_market_analysis(analysis):\n",
        "    with open('market_analysis.txt') as f:\n",
        "        for line in f:\n",
        "            fields = line.split()\n",
        "            record = (str(fields[0]),\n",
        "                      int(fields[1]),\n",
        "                      int(fields[2]),\n",
        "                      int(fields[3]),)\n",
        "            analysis.append(record)\n",
        "\n",
        "\n",
        "# CONSTANTS FOR COLUMN NAMES\n",
        "COMPANY_NAME = 0\n",
        "YEAR = 1\n",
        "DAY = 2\n",
        "QUARTER = 3\n",
        "STOCK_PRICE = 4\n",
        "EXPERT_1 = 5\n",
        "# EXPERT_2 index not used, skip\n",
        "SENTIMENT = 7\n",
        "M1 = 8\n",
        "M2 = 9\n",
        "M3 = 10\n",
        "M4 = 11\n",
        "SEGMENT = 12\n",
        "TREND = 13\n",
        "\n",
        "stock_prices = []\n",
        "info_array = []\n",
        "segments_array = []\n",
        "analysis_array = []\n",
        "\n",
        "combined_data = []\n",
        "losses = []\n",
        "\n",
        "# Store the Max value for each data\n",
        "max_stock_price = 0\n",
        "max_day = 0\n",
        "max_sentiment_analysis = 0\n",
        "max_m1 = 0\n",
        "max_m2 = 0\n",
        "\n",
        "# The target predictions for the ANN\n",
        "target_label = []\n",
        "\n",
        "# Read from the files\n",
        "read_stock_prices(stock_prices)\n",
        "read_info(info_array)\n",
        "read_segments(segments_array)\n",
        "read_market_analysis(analysis_array)\n",
        "\n",
        "# This is used to compare prev. stock price to current one\n",
        "previous = tuple()\n",
        "\n",
        "for i in range(len(stock_prices)):\n",
        "\n",
        "  ms_index = tuple() # Market segment index, instead of string\n",
        "  trend = tuple()    # Trend value, -1 to 1\n",
        "\n",
        "  if segments_array[stock_prices[i][0]][1] == 'IT':\n",
        "      ms_index = ms_index + (1,)\n",
        "      # Look up the trend value for the given segment\n",
        "      for item in analysis_array:\n",
        "          list_item = list(item)\n",
        "          if (stock_prices[i][YEAR] + 2017) == list_item[1] and \\\n",
        "                stock_prices[i][QUARTER] == list_item[2] and \\\n",
        "                list_item[0] == 'IT':\n",
        "              trend = trend + (item[3],)\n",
        "              break\n",
        "\n",
        "\n",
        "  elif segments_array[stock_prices[i][0]][1] == 'BIO':\n",
        "      ms_index = ms_index + (0,)\n",
        "      # Look up the trend value for the given segment\n",
        "      for item in analysis_array:\n",
        "          list_item = list(item)\n",
        "          if (stock_prices[i][YEAR] + 2017) == list_item[1] and \\\n",
        "                stock_prices[i][QUARTER] == list_item[2] and \\\n",
        "                list_item[0] == 'BIO':\n",
        "              trend = trend + (item[3],)\n",
        "              break\n",
        "\n",
        "  # Place all the data into a single list\n",
        "  combined_data.append(\n",
        "      stock_prices[i] +\n",
        "      info_array[i][STOCK_PRICE::] +\n",
        "      ms_index +\n",
        "      trend\n",
        "  )\n",
        "\n",
        "  # Create a lable to use for the model\n",
        "  if previous and previous[STOCK_PRICE] < stock_prices[i][STOCK_PRICE]:\n",
        "      target_label.append(1)\n",
        "  else:\n",
        "      target_label.append(0)\n",
        "\n",
        "  # See if there is a new max stock prize\n",
        "  if combined_data[i][STOCK_PRICE] > max_stock_price:\n",
        "      max_stock_price = combined_data[i][STOCK_PRICE]\n",
        "\n",
        "  if combined_data[i][DAY] > max_day:\n",
        "      max_day = combined_data[i][DAY]\n",
        "\n",
        "  if combined_data[i][SENTIMENT] > max_sentiment_analysis:\n",
        "      max_sentiment_analysis = combined_data[i][SENTIMENT]\n",
        "\n",
        "  if combined_data[i][M1] > max_m1:\n",
        "      max_m1 = combined_data[i][M1]\n",
        "\n",
        "  if combined_data[i][M2] > max_m2:\n",
        "      max_m2 = combined_data[i][M2]\n",
        "  # To compare the current stock with previous stock\n",
        "  previous = combined_data[i]\n",
        "\n",
        "# Normalize the data\n",
        "\n",
        "normalized = []\n",
        "\n",
        "for s in combined_data:\n",
        "    replace = (\n",
        "        s[COMPANY_NAME],                      # company name\n",
        "        s[YEAR],                              # year\n",
        "        s[DAY] / max_day,                     # day\n",
        "        s[QUARTER],                           # quarter\n",
        "        s[STOCK_PRICE] / max_stock_price,     # stock price\n",
        "        s[EXPERT_1],                          # expert 1\n",
        "        s[SENTIMENT] / max_sentiment_analysis,# sentiment analysis\n",
        "        s[M1] / max_m1,                       # m1\n",
        "        s[M2] / max_m2,                       # m2\n",
        "        s[M3],                                # m3\n",
        "        s[M4],                                # m4\n",
        "        s[SEGMENT],                           # market_segment, just seems to decrease accuracy\n",
        "        s[TREND],                             # trend\n",
        "    )\n",
        "    normalized.append(replace)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z98HQ1YhgFjY"
      },
      "source": [
        "# Read test and training data from files into a dataset\n",
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "BATCH_SIZE = 10 \n",
        "\n",
        "\n",
        "X, y = torch.Tensor(normalized), torch.Tensor(target_label)\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True)\n",
        "\n",
        "\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "\n",
        "\n",
        "train_set = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_set = torch.utils.data.DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZgohiLtz9AU"
      },
      "source": [
        "WIDTH = 64  # The width of the layers\n",
        "IN_FEATURES = X.shape[1]  # The amount of data coming in [x, y, z, a]\n",
        "OUT_FEATURES = 2  # Number of possible answers [0, 1]\n",
        "EPOCHS_VAL = 90\n",
        "LEARNING_RATE = 0.00515\n",
        "DROPOUT_VALUE = 0.375"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W42PgbwQPdOX"
      },
      "source": [
        "class MyANN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(DROPOUT_VALUE)\n",
        "\n",
        "        self.fc1 = nn.Linear(IN_FEATURES, WIDTH)\n",
        "        self.fc2 = nn.Linear(WIDTH, WIDTH)\n",
        "        self.fc3 = nn.Linear(WIDTH, WIDTH)\n",
        "        self.fc4 = nn.Linear(WIDTH, WIDTH)\n",
        "        self.fc5 = nn.Linear(WIDTH, OUT_FEATURES)\n",
        "        return\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.dropout(self.fc2(x)))\n",
        "        x = torch.relu(self.dropout(self.fc3(x)))\n",
        "        x = torch.relu(self.dropout(self.fc4(x)))\n",
        "        x = self.fc5(x)\n",
        "        return nn.functional.logsigmoid(x)\n",
        "\n",
        "net = MyANN()\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bzErScpOUhhL"
      },
      "source": [
        "## Train the network\n",
        "\n",
        "The model gives an accuracy of around 87.5 - 91.5%"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQKPKC5sUf1j"
      },
      "source": [
        "\n",
        "net = MyANN()\n",
        "optimization = torch.optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "net.train()\n",
        "for epochs in range(EPOCHS_VAL):\n",
        "    for data in train_set:\n",
        "        _X, _y = data\n",
        "        net.zero_grad()\n",
        "        output = net(_X)  # Forward pass\n",
        "        _y = torch.tensor(_y, dtype=torch.long)\n",
        "        loss_c = nn.CrossEntropyLoss()\n",
        "        loss = loss_c(output, _y)  # Computing\n",
        "        loss.backward()  # Back-propigation\n",
        "        optimization.step()\n",
        "    losses.append(loss.item())\n",
        "\n",
        "# Evaluate\n",
        "total = 0\n",
        "correct = 0\n",
        "net.eval()\n",
        "for data in test_set:\n",
        "    _X, _y = data\n",
        "    output = net(_X)  # Forward pass\n",
        "    for idx, val in enumerate(output):\n",
        "        if torch.argmax(val) == _y[idx]:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "print(\"Accuracy: \", round(correct / total, 3))\n",
        "\n",
        "if(round(correct / total, 3) >= 0.9): # Save all networks that give > 90% acc, muhaha\n",
        "  filename = \"network_\" + str(round(correct / total, 3)) + \".pt\"\n",
        "  print(filename)\n",
        "  torch.save(net.state_dict(), filename)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}